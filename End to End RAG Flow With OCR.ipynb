{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c14f7-73b7-4965-9bde-ba457719aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama_index.llms.azure_openai\n",
    "# !pip install llama_index.embeddings.azure_openai\n",
    "# !pip install python-dotenv\n",
    "# !pip install pymupdf\n",
    "# !pip install azure\n",
    "# !pip install azure-ai-documentintelligence\n",
    "# !pip install surya-ocr\n",
    "# !pip install pytesseract\n",
    "# !pip install pandas\n",
    "# !pip install llama-index llama-index-experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658328f7-501e-4004-80a8-88bd640004f8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# End to End RAG Flow With OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f406cd7-37c3-423d-ad88-725472a26946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from surya.ordering import batch_ordering\n",
    "from surya.model.ordering.processor import load_processor\n",
    "from surya.model.ordering.model import load_model\n",
    "import pytesseract  # For OCR\n",
    "\n",
    "# Function to perform OCR using Tesseract and layout ordering using Surya\n",
    "def ocr_with_surya(image_path):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Perform OCR using Tesseract to extract text\n",
    "    ocr_text = pytesseract.image_to_string(image)\n",
    "\n",
    "    # Dummy bounding boxes (in practice, you'd get this from a layout model)\n",
    "    # You can replace this with an actual layout model that detects bboxes\n",
    "    bboxes = [[0, 0, image.size[0], image.size[1]]]\n",
    "\n",
    "    # Load the Surya ordering model and processor\n",
    "    model = load_model()\n",
    "    processor = load_processor()\n",
    "\n",
    "    # Perform layout ordering on the image\n",
    "    order_predictions = batch_ordering([image], [bboxes], model, processor)\n",
    "\n",
    "    # Return the OCR text and layout predictions\n",
    "    return ocr_text, order_predictions\n",
    "\n",
    "# Example usage\n",
    "ocr_text, layout_order = ocr_with_surya(\"./data/passport.png\")  # Using an image of the document\n",
    "\n",
    "# Now use the extracted OCR text in your document processing\n",
    "from llama_index.core import Document\n",
    "document = Document(text=ocr_text)\n",
    "\n",
    "# Use a text splitter (SentenceSplitter) to chunk the document into nodes\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=50)\n",
    "nodes = splitter.get_nodes_from_documents([document])\n",
    "\n",
    "# Create a vector index from the nodes\n",
    "from llama_index.core import GPTVectorStoreIndex\n",
    "index = GPTVectorStoreIndex.from_documents([document])\n",
    "\n",
    "# Query the index\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"Who does the passport belong to and what is the nationality of the person?\")\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cce41b2-bbb8-43d4-ba4e-3f1aa181d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "from surya.ordering import batch_ordering\n",
    "from surya.model.ordering.processor import load_processor\n",
    "from surya.model.ordering.model import load_model\n",
    "\n",
    "# Function to extract images from PDF and apply OCR with Surya\n",
    "def ocr_with_surya_for_pdf(pdf_path):\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    ocr_results = []\n",
    "\n",
    "    # Iterate through the pages of the PDF\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "\n",
    "        # Extract images from the page\n",
    "        image_list = page.get_images(full=True)\n",
    "        for img_index, img_info in enumerate(image_list):\n",
    "            xref = img_info[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "            # Perform OCR on the extracted image\n",
    "            ocr_text = pytesseract.image_to_string(image)\n",
    "\n",
    "            # Dummy bounding boxes for layout ordering\n",
    "            bboxes = [[0, 0, image.size[0], image.size[1]]]\n",
    "\n",
    "            # Load the Surya ordering model and processor\n",
    "            model = load_model()\n",
    "            processor = load_processor()\n",
    "\n",
    "            # Perform layout ordering on the image\n",
    "            order_predictions = batch_ordering([image], [bboxes], model, processor)\n",
    "\n",
    "            # Append OCR text and layout ordering results\n",
    "            ocr_results.append({\n",
    "                \"page\": page_num + 1,\n",
    "                \"image_index\": img_index + 1,\n",
    "                \"ocr_text\": ocr_text,\n",
    "                \"layout_order\": order_predictions\n",
    "            })\n",
    "\n",
    "    return ocr_results\n",
    "\n",
    "# Example usage with a PDF file containing scanned images\n",
    "pdf_ocr_results = ocr_with_surya_for_pdf(\"./data/mai.pdf\")\n",
    "\n",
    "# Combine OCR results into a single text\n",
    "combined_ocr_text = \"\\n\\n\".join([result['ocr_text'] for result in pdf_ocr_results])\n",
    "\n",
    "# Now use the combined OCR text in your document processing\n",
    "from llama_index.core import Document\n",
    "document = Document(text=combined_ocr_text)\n",
    "\n",
    "# Use a text splitter (SentenceSplitter) to chunk the document into nodes\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=50)\n",
    "nodes = splitter.get_nodes_from_documents([document])\n",
    "\n",
    "# Create a vector index from the nodes\n",
    "from llama_index.core import GPTVectorStoreIndex\n",
    "index = GPTVectorStoreIndex.from_documents([document])\n",
    "\n",
    "# Query the index\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"What is this program and what university is offering it?\")\n",
    "# print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
